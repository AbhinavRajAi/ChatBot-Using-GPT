{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q9txdxksc3l4","executionInfo":{"status":"ok","timestamp":1708683414088,"user_tz":-330,"elapsed":7953,"user":{"displayName":"ABHINAV RAJ","userId":"02351614355683996958"}},"outputId":"efe8ebef-f257-42ef-e9b2-dfedb6fac7d0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting accelerate\n","  Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Installing collected packages: accelerate\n","Successfully installed accelerate-0.27.2\n"]}],"source":["pip install accelerate"]},{"cell_type":"code","source":["import torch\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AdamW\n","from tqdm.auto import tqdm\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, file_path, tokenizer, max_length=128):\n","        self.examples = []\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n","            for line in file:\n","                parts = line.strip().split(\"\\t\")\n","                if len(parts) != 2:\n","                    print(f\"Skipping line: {line.strip()}. Expected format: question<TAB>response\")\n","                    continue\n","\n","                question, response = parts\n","                input_text = f\"Question: {question} Answer: {response}\"\n","\n","                # Tokenize and pad input\n","                tokenized_input = self.tokenizer(input_text, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n","                if \"input_ids\" not in tokenized_input or \"attention_mask\" not in tokenized_input:\n","                    print(f\"Skipping line: {line.strip()}. Tokenization failed.\")\n","                    continue\n","\n","                self.examples.append(tokenized_input)\n","\n","    def __len__(self):\n","        return len(self.examples)\n","\n","    def __getitem__(self, idx):\n","        return self.examples[idx]\n","\n","# Load pre-trained GPT-2 model and tokenizer\n","model_name = \"gpt2\"\n","tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","tokenizer.pad_token = tokenizer.eos_token  # Set pad_token to eos_token\n","model = GPT2LMHeadModel.from_pretrained(model_name)\n","\n","\n","# Load your dataset\n","train_path = \"/content/drive/MyDrive/dialogs.txt\"\n","train_dataset = CustomDataset(train_path, tokenizer)\n","\n","# Define optimizer\n","optimizer = AdamW(model.parameters(), lr=5e-5)\n","\n","# Training loop\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","model.train()\n","\n","epochs = 30\n","for epoch in range(epochs):\n","    total_loss = 0\n","    for batch in tqdm(DataLoader(train_dataset, batch_size=8, shuffle=True)):\n","        optimizer.zero_grad()\n","        input_ids = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","        labels = input_ids.clone()\n","        labels[labels == tokenizer.pad_token_id] = -100  # Ignore loss on padding tokens\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    print(f\"Epoch {epoch+1}, Loss: {total_loss}\")\n","\n","\n","# Save the fine-tuned model\n","model.save_pretrained(\"/content/drive/MyDrive/gpt2-finetuned_m1\")\n","tokenizer.save_pretrained(\"/content/drive/MyDrive/gpt2-finetuned1\")\n"],"metadata":{"id":"how5fHv1d0Ox"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Ez55bu2b1EiP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","\n","# Load pre-trained model and tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained(\"/content/drive/MyDrive/gpt2-finetuned\")\n","model = GPT2LMHeadModel.from_pretrained(\"/content/drive/MyDrive/gpt2-finetuned_m\")\n","\n","# Set the model to evaluation mode\n","model.eval()\n","\n","def generate_response(question, max_length=40):\n","    # Encode the user's question to prepare it for the model\n","    input_ids = tokenizer.encode(question, return_tensors=\"pt\")\n","\n","    # Generate a response with adjusted parameters for better control\n","    with torch.no_grad():\n","        output = model.generate(\n","            input_ids,\n","            max_length=max_length,\n","            num_return_sequences=1,\n","            num_beams=5,  # Use beam search with 5 beams\n","            early_stopping=True,  # Enable early stopping\n","            no_repeat_ngram_size=1,  # Prevent repeating n-grams\n","            eos_token_id=tokenizer.eos_token_id,  # Specify EOS token for stopping the generation\n","            pad_token_id=tokenizer.eos_token_id,  # Ensure correct padding token is used\n","        )\n","\n","    # Decode the generated response\n","    response = tokenizer.decode(output[0], skip_special_tokens=True)\n","    return response\n","\n","\n","\n","\n","# Start conversation\n","print(\"Bot: Hi! I'm a chatbot. You can start by asking me anything.\")\n","while True:\n","    user_input = input(\"You: \")\n","    if user_input.lower() in ['exit', 'quit']:\n","        print(\"Bot: Goodbye!\")\n","        break\n","    response = generate_response(user_input)\n","    print(\"Bot:\", response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BCtOQnC0eCIo","executionInfo":{"status":"ok","timestamp":1708690522933,"user_tz":-330,"elapsed":267190,"user":{"displayName":"ABHINAV RAJ","userId":"02351614355683996958"}},"outputId":"6ed5d508-9b62-4a83-f818-7bada01c281a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Bot: Hi! I'm a chatbot. You can start by asking me anything.\n","You: hi, how are you ?\n","Bot: hi, how are you? Answer: i've been working a lot lately. what's going on? is something wrong with my computer.? why not just type in the email address and see if it\n","You: hi, how are you doing?\n","Bot: hi, how are you doing? Answer: i've been working a lot lately. what's the weather going to be like in about 90 minutes or so.? it depends on your point of view and\n","You: i know. i think it may rain.\n","Bot: i know. i think it may rain. Answer: what's the weather going to be like? we'll have hot and sunny days every sunday, except on friday or thirteenth.?\n","You: exit\n","Bot: Goodbye!\n"]}]}]}